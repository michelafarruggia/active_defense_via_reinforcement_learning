{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, ProgressBarCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "from baselines import logger\n",
    "\n",
    "# Import Yawning Titan specific modules\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.game_modes.game_mode import GameMode\n",
    "from yawning_titan.networks.node import Node\n",
    "from yawning_titan.networks.network import Network\n",
    "from yawning_titan.envs.generic.core import reward_functions\n",
    "from yawning_titan.envs.generic.helpers.eval_printout import EvalPrintout\n",
    "from yawning_titan.envs.generic.helpers.graph2plot import CustomEnvGraph\n",
    "\n",
    "# Import Reward Machine related modules\n",
    "from reward_machines.reward_machine import RewardMachine\n",
    "from reward_machines.rm_environment import RewardMachineEnv, RewardMachineWrapper\n",
    "from reward_machines.reward_functions import ConstantRewardFunction\n",
    "from reward_machines.reward_machine_utils import evaluate_dnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "UUID                                  Name                           High Value Node    Entry Node      Vulnerability  Position (x,y)\n",
      "------------------------------------  -----------------------------  -----------------  ------------  ---------------  ----------------\n",
      "c69d05cd-13f2-42ed-93a0-d1be83b0f3b8  Boundary router packet filter  False              True                     0.01  0.00, 600.00\n",
      "491ae9b2-c1d5-4982-a165-0a1a2bf6fff4  External switch                False              False                    0.01  0.00, 500.00\n",
      "2f2552f8-e713-4841-828e-78563be4fa37  Network IDS (DMZ)              False              False                    0.01  -200.00, 500.00\n",
      "fad24705-3b1b-4c82-9859-90e8a1f1bb0e  DNS server esterno             False              False                    0.01  200.00, 500.00\n",
      "5272a0dd-e046-4938-baf0-7212cac58e48  Main Firewall/VPN server/NAT   False              False                    0.01  -100.00, 400.00\n",
      "88e7c46f-67dd-4d94-be0c-bc7ca9d3c335  External Web Server/Host IDS   False              False                    0.01  100.00, 400.00\n",
      "bbbdf613-83bb-474d-85e2-59618a295865  Internal Switch (DMZ)          False              False                    0.01  0.00, 300.00\n",
      "eb8ad93a-50de-4227-aeee-05ab4b9f5161  Network IDS (DMZ Internal)     False              False                    0.01  -200.00, 300.00\n",
      "7acd53da-fbcf-427a-a413-577da291d204  Internal Firewall              False              False                    0.01  0.00, 200.00\n",
      "c8f8f3f2-7c59-409e-b110-d9dc1018b860  Database Server                True               False                    0.01  200.00, 200.00\n",
      "40fdfd91-67da-41b2-afdd-16ad6ac057e8  Email Server/Host IDS          False              False                    0.01  -200.00, 200.00\n",
      "c3c4bbc5-7d5c-4894-b93b-7fcbf9c933f1  DNS server interno             False              False                    0.01  400.00, 200.00\n",
      "deec8794-2053-4409-8f9e-b13016672bdb  Web proxy server               False              False                    0.01  -400.00, 200.00\n",
      "1b19a86c-ef9d-4f6d-9469-65f727ab781b  Internal Switch                False              False                    0.01  0.00, 100.00\n",
      "2841693f-f84c-44ad-88d3-a391332bd731  Network IDS Internal           False              False                    0.01  -200.00, 100.00\n",
      "9bdda85e-9a40-408d-bdf9-da3f0100ba39  Router Subnet 1                False              False                    0.01  -300.00, 0.00\n",
      "31de1ad8-7bcc-462b-9a27-797513f161d3  Router Subnet 2                False              False                    0.01  -100.00, 0.00\n",
      "e09d8692-0f1d-4907-a273-fafdaab79839  Router Subnet 3                False              False                    0.01  100.00, 0.00\n",
      "86e46e8e-715e-49a8-8276-fbcae20203b2  Router Subnet 4                False              False                    0.01  300.00, 0.00\n",
      "386c746d-88ca-4fc7-9fe0-2c02e8e93b40  Client1 Management             False              False                    0.01  -300.00, -100.00\n",
      "add598bb-a6f1-4259-bfa9-87ad773f5280  Client1 HR                     False              False                    0.01  -100.00, -100.00\n",
      "e8f432f2-f9fd-4048-b45f-6077d2472bc2  Client1 IT                     False              False                    0.01  100.00, -100.00\n",
      "c3a601ee-270f-4cc1-9880-4c06148da2e2  Server backup                  True               False                    0.01  300.00, -100.00\n"
     ]
    }
   ],
   "source": [
    "# Load the game mode from a YAML file\n",
    "game_mode = GameMode()\n",
    "game_mode = game_mode.create_from_yaml(yaml='CONFIG YAML FILE', legacy=False, infer_legacy=True)\n",
    "print(game_mode.game_rules.max_steps.value)\n",
    "\n",
    "# Create a network representation\n",
    "network = Network()\n",
    "\n",
    "# Define network nodes and their positions\n",
    "\n",
    "# External layer\n",
    "router_1 = Node(\"Boundary router packet filter\")\n",
    "router_1.node_position = [0, 600]\n",
    "network.add_node(router_1)\n",
    "\n",
    "switch_1 = Node(\"External switch\")\n",
    "switch_1.node_position = [0, 500]\n",
    "network.add_node(switch_1)\n",
    "\n",
    "network_ids_1 = Node(\"Network IDS (DMZ)\")\n",
    "network_ids_1.node_position = [-200, 500]\n",
    "network.add_node(network_ids_1)\n",
    "\n",
    "dns_server_external = Node(\"DNS server esterno\")\n",
    "dns_server_external.node_position = [200, 500]\n",
    "network.add_node(dns_server_external)\n",
    "\n",
    "# DMZ layer\n",
    "server_1 = Node(\"Main Firewall/VPN server/NAT\")\n",
    "server_1.node_position = [-100, 400]\n",
    "network.add_node(server_1)\n",
    "\n",
    "server_2 = Node(\"External Web Server/Host IDS\")\n",
    "server_2.node_position = [100, 400]\n",
    "network.add_node(server_2)\n",
    "\n",
    "switch_2 = Node(\"Internal Switch (DMZ)\")\n",
    "switch_2.node_position = [0, 300]\n",
    "network.add_node(switch_2)\n",
    "\n",
    "network_ids_2 = Node(\"Network IDS (DMZ Internal)\")\n",
    "network_ids_2.node_position = [-200, 300]\n",
    "network.add_node(network_ids_2)\n",
    "\n",
    "# Internal layer\n",
    "internal_firewall = Node(\"Internal Firewall\")\n",
    "internal_firewall.node_position = [0, 200]\n",
    "network.add_node(internal_firewall)\n",
    "\n",
    "server_3 = Node(\"Database Server\")\n",
    "server_3.node_position = [200, 200]\n",
    "network.add_node(server_3)\n",
    "\n",
    "email_server = Node(\"Email Server/Host IDS\")\n",
    "email_server.node_position = [-200, 200]\n",
    "network.add_node(email_server)\n",
    "\n",
    "dns_server_internal = Node(\"DNS server interno\")\n",
    "dns_server_internal.node_position = [400, 200]\n",
    "network.add_node(dns_server_internal)\n",
    "\n",
    "web_proxy = Node(\"Web proxy server\")\n",
    "web_proxy.node_position = [-400, 200]\n",
    "network.add_node(web_proxy)\n",
    "\n",
    "switch_3 = Node(\"Internal Switch\")\n",
    "switch_3.node_position = [0, 100]\n",
    "network.add_node(switch_3)\n",
    "\n",
    "network_ids_3 = Node(\"Network IDS Internal\")\n",
    "network_ids_3.node_position = [-200, 100]\n",
    "network.add_node(network_ids_3)\n",
    "\n",
    "# Subnet layer\n",
    "subnet1_router = Node(\"Router Subnet 1\")\n",
    "subnet1_router.node_position = [-300, 0]\n",
    "network.add_node(subnet1_router)\n",
    "\n",
    "subnet2_router = Node(\"Router Subnet 2\")\n",
    "subnet2_router.node_position = [-100, 0]\n",
    "network.add_node(subnet2_router)\n",
    "\n",
    "subnet3_router = Node(\"Router Subnet 3\")\n",
    "subnet3_router.node_position = [100, 0]\n",
    "network.add_node(subnet3_router)\n",
    "\n",
    "subnet4_router = Node(\"Router Subnet 4\")\n",
    "subnet4_router.node_position = [300, 0]\n",
    "network.add_node(subnet4_router)\n",
    "\n",
    "# Client/Server layer\n",
    "client1_management = Node(\"Client1 Management\")\n",
    "client1_management.node_position = [-300, -100]\n",
    "network.add_node(client1_management)\n",
    "\n",
    "client1_hr = Node(\"Client1 HR\")\n",
    "client1_hr.node_position = [-100, -100]\n",
    "network.add_node(client1_hr)\n",
    "\n",
    "client1_it = Node(\"Client1 IT\")\n",
    "client1_it.node_position = [100, -100]\n",
    "network.add_node(client1_it)\n",
    "\n",
    "server_backup = Node(\"Server backup\")\n",
    "server_backup.node_position = [300, -100]\n",
    "network.add_node(server_backup)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "network.add_edge(router_1, switch_1)\n",
    "network.add_edge(switch_1, server_1)\n",
    "network.add_edge(switch_1, server_2)\n",
    "network.add_edge(switch_1, network_ids_1)\n",
    "network.add_edge(switch_1, dns_server_external)\n",
    "network.add_edge(server_1, switch_2)\n",
    "network.add_edge(switch_2, server_3)\n",
    "network.add_edge(switch_2, internal_firewall)\n",
    "network.add_edge(switch_2, email_server)\n",
    "network.add_edge(switch_2, dns_server_internal)\n",
    "network.add_edge(switch_2, web_proxy)\n",
    "network.add_edge(switch_2, network_ids_2)\n",
    "network.add_edge(internal_firewall, switch_3)\n",
    "network.add_edge(switch_3, network_ids_3)\n",
    "network.add_edge(switch_3, subnet1_router)\n",
    "network.add_edge(switch_3, subnet2_router)\n",
    "network.add_edge(switch_3, subnet3_router)\n",
    "network.add_edge(switch_3, subnet4_router)\n",
    "network.add_edge(subnet1_router, client1_management)\n",
    "network.add_edge(subnet2_router, client1_hr)\n",
    "network.add_edge(subnet3_router, client1_it)\n",
    "network.add_edge(subnet4_router, server_backup)\n",
    "\n",
    "\n",
    "# Set entry and high-value nodes\n",
    "router_1.entry_node = True\n",
    "server_3.high_value_node = True\n",
    "server_backup.high_value_node = True\n",
    "\n",
    "\n",
    "# Display the network details\n",
    "network.show(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkInterface, RedInterface, and BlueInterface objects\n",
    "network_interface = NetworkInterface(game_mode=game_mode, network=network)\n",
    "red = RedInterface(network_interface)\n",
    "blue = BlueInterface(network_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YawningTitanRMEnv(RewardMachineEnv):\n",
    "    def __init__(self, env, rm_files):\n",
    "        super().__init__(env, rm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YawningTitanTest:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A generic class that creates Open AI environments within YAWNING TITAN.\n",
    "\n",
    "This class has several key inputs which determine aspects of the environment such\n",
    "as how the red agent behaves, what the red team and blue team objectives are, the size\n",
    "and topology of the network being defended and what data should be collected during the simulation.\n",
    "\"\"\"\n",
    "\n",
    "class GenericNetworkRMEnv(gym.Env):\n",
    "    \"\"\"Class to create a generic YAWNING TITAN gym environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        red_agent: RedInterface,\n",
    "        blue_agent: BlueInterface,\n",
    "        network_interface: NetworkInterface,\n",
    "        rm_files,\n",
    "        print_metrics: bool = False,\n",
    "        show_metrics_every: int = 1,\n",
    "        collect_additional_per_ts_data: bool = True,\n",
    "        print_per_ts_data: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the generic network environment.\n",
    "\n",
    "        Args:\n",
    "            env: The base environment\n",
    "            red_agent: Object from the RedInterface class\n",
    "            blue_agent: Object from the BlueInterface class\n",
    "            network_interface: Object from the NetworkInterface class\n",
    "            rm_files: List of reward machine files\n",
    "            print_metrics: Whether or not to print metrics (boolean)\n",
    "            show_metrics_every: Number of timesteps to show summary metrics (int)\n",
    "            collect_additional_per_ts_data: Whether or not to collect additional per timestep data (boolean)\n",
    "            print_per_ts_data: Whether or not to print collected per timestep data (boolean)\n",
    "        \"\"\"\n",
    "        super(GenericNetworkRMEnv, self).__init__()\n",
    "        self.env = env\n",
    "        self.RED = red_agent\n",
    "        self.BLUE = blue_agent\n",
    "        self.blue_actions = blue_agent.get_number_of_actions()\n",
    "        self.network_interface = network_interface\n",
    "        self.rm_files = rm_files\n",
    "        self.current_duration = 0\n",
    "        self.game_stats_list = []\n",
    "        self.num_games_since_avg = 0\n",
    "        self.avg_every = show_metrics_every\n",
    "        self.current_game_blue = {}\n",
    "        self.current_game_stats = {}\n",
    "        self.total_games = 0\n",
    "        self.made_safe_nodes = []\n",
    "        self.current_reward = 0\n",
    "        self.print_metrics = print_metrics\n",
    "        self.print_notes = print_per_ts_data\n",
    "\n",
    "\n",
    "        self.random_seed = self.network_interface.random_seed\n",
    "\n",
    "        self.graph_plotter = None\n",
    "        self.eval_printout = EvalPrintout(self.avg_every)\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.blue_actions)\n",
    "\n",
    "        self.network_interface.get_observation_size()\n",
    "\n",
    "        # sets up the observation space. This is a (n+2 by n) matrix. The first two columns show the state of all the\n",
    "        # nodes. The remaining n columns show the connections between the nodes (effectively the adjacency matrix)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=1,\n",
    "            shape=(self.network_interface.get_observation_size(),),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # The gym environment can only properly deal with a 1d array so the observation is flattened\n",
    "\n",
    "        self.collect_data = collect_additional_per_ts_data\n",
    "        self.env_observation = self.network_interface.get_current_observation()\n",
    "    \n",
    "    def get_true_propositions(self):\n",
    "        self.true_props = \"\"\n",
    "\n",
    "        compromised = any(node.true_compromised_status for node in self.network_interface.current_graph.get_nodes())\n",
    "        max_steps_reached = self.current_duration == self.network_interface.game_mode.game_rules.max_steps.value\n",
    "        high_value_compromised = any(node.true_compromised_status for node in self.network_interface.current_graph.get_nodes() if node.high_value_node)\n",
    "\n",
    "        if compromised:\n",
    "            self.true_props += 'c'\n",
    "        if max_steps_reached:\n",
    "            self.true_props += 'm'\n",
    "        if high_value_compromised:\n",
    "            self.true_props += 'h'\n",
    "\n",
    "        return self.true_props\n",
    "    \n",
    "    def get_events(self):\n",
    "        return self.get_true_propositions()\n",
    "    \n",
    "    def execute_action(self, action: int) -> Tuple[np.array, bool, Dict[str, dict]]:\n",
    "        \"\"\"\n",
    "        Executes the 'action' in the Yawning Titan environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action to be executed by the Blue agent.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - The next observation (flattened numpy array)\n",
    "            - A boolean indicating if the episode is done\n",
    "            - Additional information (dictionary)\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        done = False\n",
    "        blue_action = \"\"\n",
    "        blue_node = None\n",
    "\n",
    "        # Store the initial state of the environment\n",
    "        initial_state = self.network_interface.get_all_node_compromised_states()\n",
    "\n",
    "        # Execute the Blue agent's action\n",
    "        blue_action, blue_node = self.BLUE.perform_action(action)\n",
    "\n",
    "        # Store the result of the Blue action\n",
    "        if blue_action == \"make_node_safe\" or blue_action == \"restore_node\":\n",
    "            self.made_safe_nodes.append(blue_node)\n",
    "\n",
    "        # Determine the Red agent's actions and update the state\n",
    "        if (\n",
    "            self.network_interface.game_mode.game_rules.grace_period_length.value\n",
    "            <= self.current_duration\n",
    "        ):\n",
    "            red_info = self.RED.perform_action()\n",
    "        else:\n",
    "            red_info = {\n",
    "                0: {\n",
    "                    \"Action\": \"do_nothing\",\n",
    "                    \"Attacking_Nodes\": [],\n",
    "                    \"Target_Nodes\": [],\n",
    "                    \"Successes\": [True],\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Update the state of the environment after the actions\n",
    "        post_red_state = self.network_interface.get_all_node_compromised_states()\n",
    "\n",
    "        # Update the observation\n",
    "        self.env_observation = self.network_interface.get_current_observation().flatten()\n",
    "        self.current_duration += 1\n",
    "\n",
    "        # Check if the episode is done (e.g., maximum steps reached, all nodes compromised)\n",
    "        if self.current_duration == self.network_interface.game_mode.game_rules.max_steps.value:\n",
    "            done = True\n",
    "\n",
    "        # Additional information to return\n",
    "        info = {\n",
    "            \"blue_action\": blue_action,\n",
    "            \"blue_node\": blue_node,\n",
    "            \"red_info\": red_info,\n",
    "            \"post_red_state\": post_red_state,\n",
    "        }\n",
    "\n",
    "        return self.env_observation, done, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.execute_action(action)\n",
    "        reward = 0 # all the reward comes from the RM\n",
    "        done = False\n",
    "        info = {}\n",
    "        return self.env_observation, reward, done, info\n",
    "\n",
    "    def reset(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Reset the environment to the default state.\n",
    "\n",
    "        :todo: May need to add customization of cuda setting.\n",
    "\n",
    "        :return: A new starting observation (numpy array).\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.random_seed is not None:  # conditionally set random_seed\n",
    "            set_random_seed(self.random_seed, True)\n",
    "        self.network_interface.reset()\n",
    "        self.RED.reset()\n",
    "        self.current_duration = 0\n",
    "        self.env_observation = self.network_interface.get_current_observation()\n",
    "        self.current_game_blue = {}\n",
    "\n",
    "        return self.env_observation\n",
    "\n",
    "  \n",
    "    def render(\n",
    "        self,\n",
    "        mode: str = \"human\",\n",
    "        show_only_blue_view: bool = False,\n",
    "        show_node_names: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Render the environment using Matplotlib to create an animation.\n",
    "\n",
    "        Args:\n",
    "            mode: the mode of the rendering\n",
    "            show_only_blue_view: If true shows only what the blue agent can see\n",
    "            show_node_names: Show the names of the nodes\n",
    "        \"\"\"\n",
    "        if self.graph_plotter is None:\n",
    "            self.graph_plotter = CustomEnvGraph()\n",
    "\n",
    "        # gets the networkx object\n",
    "\n",
    "        # compromised nodes is a dictionary of all the compromised nodes with a 1 if the compromise is known or a 0 if\n",
    "        # not\n",
    "        # gets information about the current state from the network interface\n",
    "        main_graph = self.network_interface.current_graph\n",
    "        if show_only_blue_view:\n",
    "            attacks = self.network_interface.detected_attacks\n",
    "        else:\n",
    "            attacks = self.network_interface.true_attacks\n",
    "        reward = round(self.current_reward, 2)\n",
    "\n",
    "        # sends the current information to a graph plotter to display the information visually\n",
    "        self.graph_plotter.render(\n",
    "            current_step=self.current_duration,\n",
    "            g=main_graph,\n",
    "            attacked_nodes=attacks,\n",
    "            current_time_step_reward=reward,\n",
    "            # self.network_interface.red_current_location,\n",
    "            made_safe_nodes=self.made_safe_nodes,\n",
    "            target_node=self.network_interface.get_target_node(),\n",
    "            # \"RL blue agent vs probabilistic red in a generic network environment\",\n",
    "            show_only_blue_view=show_only_blue_view,\n",
    "            show_node_names=show_node_names,\n",
    "        )\n",
    "\n",
    "    def calculate_observation_space_size(self, with_feather: bool) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the observation space size.\n",
    "\n",
    "        This is done using the current active observation space configuration\n",
    "        and the number of nodes within the environment.\n",
    "\n",
    "        Args:\n",
    "            with_feather: Whether to include the size of the Feather Wrapper output\n",
    "\n",
    "        Returns:\n",
    "            The observation space size\n",
    "        \"\"\"\n",
    "        return self.network_interface.get_observation_size_base(with_feather)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the reward machine file\n",
    "rm_files = [\"t1.txt\"]\n",
    " \n",
    "# Define the YT1RMEnv class which combines the Yawning Titan environment with the Reward Machine\n",
    "class YT1RMEnv(YawningTitanRMEnv):\n",
    "    def __init__(self):\n",
    "        env = YawningTitanTest()\n",
    "        generic_env = GenericNetworkRMEnv(\n",
    "            env,\n",
    "            red,\n",
    "            blue,\n",
    "            network_interface,\n",
    "            rm_files,\n",
    "            show_metrics_every=10\n",
    "        )\n",
    "        super().__init__(generic_env, rm_files)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n",
      "Terminal states: [2]\n",
      "Final U: [0, 1]\n",
      "delta_u: {0: {0: '!c&!m&!h', 1: 'c&!m&!h', -1: '!c&m&!h|c&m&!h'}, 1: {0: '!c&!m&!h', 1: 'c&!m&!h', -1: '!c&m&!h|c&m&!h'}}\n",
      "delta_r: {0: {0: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833ff10>, 1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833f040>, -1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833f130>}, 1: {0: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833e920>, 1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833db40>, -1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16833ec80>}}\n"
     ]
    }
   ],
   "source": [
    "YTenv = YT1RMEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create and train PPO agent \n",
    "agent = PPO(PPOMlp, YTenv, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.1     |\n",
      "|    ep_rew_mean     | 0.133    |\n",
      "| time/              |          |\n",
      "|    fps             | 1764     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.9        |\n",
      "|    ep_rew_mean          | 0.191       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1319        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011962872 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.25       |\n",
      "|    explained_variance   | -0.741      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0858     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 0.0109      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 87.3        |\n",
      "|    ep_rew_mean          | 0.229       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1135        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010914712 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.24       |\n",
      "|    explained_variance   | -0.249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0613     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.00803     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 91.4        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1030        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017027352 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.22       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0678     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    value_loss           | 0.00668     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 95.4        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1006        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015401537 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.2        |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.00689     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16833e230>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(agent, YTenv, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1\n",
      "Step 0: Events: c, Reward: 0.0\n",
      "Step 1: Events: c, Reward: 0.0\n",
      "Step 2: Events: c, Reward: 0.0\n",
      "Step 3: Events: c, Reward: 0.0\n",
      "Step 4: Events: c, Reward: 0.0\n",
      "Step 5: Events: c, Reward: 0.0\n",
      "Step 6: Events: c, Reward: 0.0\n",
      "Step 7: Events: c, Reward: 0.0\n",
      "Step 8: Events: c, Reward: 0.0\n",
      "Step 9: Events: c, Reward: 0.0\n",
      "Step 10: Events: c, Reward: 0.0\n",
      "Step 11: Events: c, Reward: 0.0\n",
      "Step 12: Events: c, Reward: 0.0\n",
      "Step 13: Events: c, Reward: 0.0\n",
      "Step 14: Events: c, Reward: 0.0\n",
      "Step 15: Events: c, Reward: 0.0\n",
      "Step 16: Events: ch, Reward: 0\n",
      "Episode finished. Total reward: 0.0\n",
      "\n",
      "Episode 2\n",
      "Step 0: Events: c, Reward: 0.0\n",
      "Step 1: Events: c, Reward: 0.0\n",
      "Step 2: Events: c, Reward: 0.0\n",
      "Step 3: Events: c, Reward: 0.0\n",
      "Step 4: Events: c, Reward: 0.0\n",
      "Step 5: Events: c, Reward: 0.0\n",
      "Step 6: Events: c, Reward: 0.0\n",
      "Step 7: Events: c, Reward: 0.0\n",
      "Step 8: Events: c, Reward: 0.0\n",
      "Step 9: Events: c, Reward: 0.0\n",
      "Step 10: Events: c, Reward: 0.0\n",
      "Step 11: Events: c, Reward: 0.0\n",
      "Step 12: Events: c, Reward: 0.0\n",
      "Step 13: Events: c, Reward: 0.0\n",
      "Step 14: Events: c, Reward: 0.0\n",
      "Step 15: Events: c, Reward: 0.0\n",
      "Step 16: Events: c, Reward: 0.0\n",
      "Step 17: Events: c, Reward: 0.0\n",
      "Step 18: Events: c, Reward: 0.0\n",
      "Step 19: Events: c, Reward: 0.0\n",
      "Step 20: Events: c, Reward: 0.0\n",
      "Step 21: Events: c, Reward: 0.0\n",
      "Step 22: Events: c, Reward: 0.0\n",
      "Step 23: Events: c, Reward: 0.0\n",
      "Step 24: Events: c, Reward: 0.0\n",
      "Step 25: Events: c, Reward: 0.0\n",
      "Step 26: Events: c, Reward: 0.0\n",
      "Step 27: Events: c, Reward: 0.0\n",
      "Step 28: Events: c, Reward: 0.0\n",
      "Step 29: Events: c, Reward: 0.0\n",
      "Step 30: Events: c, Reward: 0.0\n",
      "Step 31: Events: c, Reward: 0.0\n",
      "Step 32: Events: c, Reward: 0.0\n",
      "Step 33: Events: c, Reward: 0.0\n",
      "Step 34: Events: c, Reward: 0.0\n",
      "Step 35: Events: c, Reward: 0.0\n",
      "Step 36: Events: c, Reward: 0.0\n",
      "Step 37: Events: c, Reward: 0.0\n",
      "Step 38: Events: c, Reward: 0.0\n",
      "Step 39: Events: c, Reward: 0.0\n",
      "Step 40: Events: c, Reward: 0.0\n",
      "Step 41: Events: c, Reward: 0.0\n",
      "Step 42: Events: c, Reward: 0.0\n",
      "Step 43: Events: c, Reward: 0.0\n",
      "Step 44: Events: c, Reward: 0.0\n",
      "Step 45: Events: c, Reward: 0.0\n",
      "Step 46: Events: c, Reward: 0.0\n",
      "Step 47: Events: c, Reward: 0.0\n",
      "Step 48: Events: c, Reward: 0.0\n",
      "Step 49: Events: c, Reward: 0.0\n",
      "Step 50: Events: c, Reward: 0.0\n",
      "Step 51: Events: c, Reward: 0.0\n",
      "Step 52: Events: c, Reward: 0.0\n",
      "Step 53: Events: c, Reward: 0.0\n",
      "Step 54: Events: c, Reward: 0.0\n",
      "Step 55: Events: c, Reward: 0.0\n",
      "Step 56: Events: c, Reward: 0.0\n",
      "Step 57: Events: c, Reward: 0.0\n",
      "Step 58: Events: c, Reward: 0.0\n",
      "Step 59: Events: c, Reward: 0.0\n",
      "Step 60: Events: c, Reward: 0.0\n",
      "Step 61: Events: c, Reward: 0.0\n",
      "Step 62: Events: c, Reward: 0.0\n",
      "Step 63: Events: c, Reward: 0.0\n",
      "Step 64: Events: c, Reward: 0.0\n",
      "Step 65: Events: c, Reward: 0.0\n",
      "Step 66: Events: c, Reward: 0.0\n",
      "Step 67: Events: c, Reward: 0.0\n",
      "Step 68: Events: c, Reward: 0.0\n",
      "Step 69: Events: c, Reward: 0.0\n",
      "Step 70: Events: c, Reward: 0.0\n",
      "Step 71: Events: c, Reward: 0.0\n",
      "Step 72: Events: c, Reward: 0.0\n",
      "Step 73: Events: c, Reward: 0.0\n",
      "Step 74: Events: c, Reward: 0.0\n",
      "Step 75: Events: c, Reward: 0.0\n",
      "Step 76: Events: c, Reward: 0.0\n",
      "Step 77: Events: c, Reward: 0.0\n",
      "Step 78: Events: c, Reward: 0.0\n",
      "Step 79: Events: c, Reward: 0.0\n",
      "Step 80: Events: c, Reward: 0.0\n",
      "Step 81: Events: c, Reward: 0.0\n",
      "Step 82: Events: c, Reward: 0.0\n",
      "Step 83: Events: c, Reward: 0.0\n",
      "Step 84: Events: c, Reward: 0.0\n",
      "Step 85: Events: c, Reward: 0.0\n",
      "Step 86: Events: c, Reward: 0.0\n",
      "Step 87: Events: c, Reward: 0.0\n",
      "Step 88: Events: c, Reward: 0.0\n",
      "Step 89: Events: c, Reward: 0.0\n",
      "Step 90: Events: c, Reward: 0.0\n",
      "Step 91: Events: c, Reward: 0.0\n",
      "Step 92: Events: c, Reward: 0.0\n",
      "Step 93: Events: c, Reward: 0.0\n",
      "Step 94: Events: c, Reward: 0.0\n",
      "Step 95: Events: c, Reward: 0.0\n",
      "Step 96: Events: c, Reward: 0.0\n",
      "Step 97: Events: c, Reward: 0.0\n",
      "Step 98: Events: c, Reward: 0.0\n",
      "Step 99: Events: c, Reward: 0.0\n",
      "Step 100: Events: c, Reward: 0.0\n",
      "Step 101: Events: c, Reward: 0.0\n",
      "Step 102: Events: c, Reward: 0.0\n",
      "Step 103: Events: c, Reward: 0.0\n",
      "Step 104: Events: c, Reward: 0.0\n",
      "Step 105: Events: c, Reward: 0.0\n",
      "Step 106: Events: c, Reward: 0.0\n",
      "Step 107: Events: c, Reward: 0.0\n",
      "Step 108: Events: c, Reward: 0.0\n",
      "Step 109: Events: c, Reward: 0.0\n",
      "Step 110: Events: c, Reward: 0.0\n",
      "Step 111: Events: c, Reward: 0.0\n",
      "Step 112: Events: c, Reward: 0.0\n",
      "Step 113: Events: c, Reward: 0.0\n",
      "Step 114: Events: c, Reward: 0.0\n",
      "Step 115: Events: c, Reward: 0.0\n",
      "Step 116: Events: c, Reward: 0.0\n",
      "Step 117: Events: c, Reward: 0.0\n",
      "Step 118: Events: c, Reward: 0.0\n",
      "Step 119: Events: c, Reward: 0.0\n",
      "Step 120: Events: c, Reward: 0.0\n",
      "Step 121: Events: c, Reward: 0.0\n",
      "Step 122: Events: c, Reward: 0.0\n",
      "Step 123: Events: c, Reward: 0.0\n",
      "Step 124: Events: c, Reward: 0.0\n",
      "Step 125: Events: c, Reward: 0.0\n",
      "Step 126: Events: c, Reward: 0.0\n",
      "Step 127: Events: c, Reward: 0.0\n",
      "Step 128: Events: c, Reward: 0.0\n",
      "Step 129: Events: c, Reward: 0.0\n",
      "Step 130: Events: c, Reward: 0.0\n",
      "Step 131: Events: c, Reward: 0.0\n",
      "Step 132: Events: c, Reward: 0.0\n",
      "Step 133: Events: c, Reward: 0.0\n",
      "Step 134: Events: c, Reward: 0.0\n",
      "Step 135: Events: c, Reward: 0.0\n",
      "Step 136: Events: c, Reward: 0.0\n",
      "Step 137: Events: c, Reward: 0.0\n",
      "Step 138: Events: c, Reward: 0.0\n",
      "Step 139: Events: c, Reward: 0.0\n",
      "Step 140: Events: c, Reward: 0.0\n",
      "Step 141: Events: c, Reward: 0.0\n",
      "Step 142: Events: c, Reward: 0.0\n",
      "Step 143: Events: c, Reward: 0.0\n",
      "Step 144: Events: c, Reward: 0.0\n",
      "Step 145: Events: c, Reward: 0.0\n",
      "Step 146: Events: c, Reward: 0.0\n",
      "Step 147: Events: c, Reward: 0.0\n",
      "Step 148: Events: c, Reward: 0.0\n",
      "Step 149: Events: c, Reward: 0.0\n",
      "Step 150: Events: c, Reward: 0.0\n",
      "Step 151: Events: c, Reward: 0.0\n",
      "Step 152: Events: c, Reward: 0.0\n",
      "Step 153: Events: c, Reward: 0.0\n",
      "Step 154: Events: c, Reward: 0.0\n",
      "Step 155: Events: c, Reward: 0.0\n",
      "Step 156: Events: c, Reward: 0.0\n",
      "Step 157: Events: c, Reward: 0.0\n",
      "Step 158: Events: c, Reward: 0.0\n",
      "Step 159: Events: c, Reward: 0.0\n",
      "Step 160: Events: c, Reward: 0.0\n",
      "Step 161: Events: c, Reward: 0.0\n",
      "Step 162: Events: c, Reward: 0.0\n",
      "Step 163: Events: c, Reward: 0.0\n",
      "Step 164: Events: c, Reward: 0.0\n",
      "Step 165: Events: c, Reward: 0.0\n",
      "Step 166: Events: c, Reward: 0.0\n",
      "Step 167: Events: c, Reward: 0.0\n",
      "Step 168: Events: c, Reward: 0.0\n",
      "Step 169: Events: c, Reward: 0.0\n",
      "Step 170: Events: c, Reward: 0.0\n",
      "Step 171: Events: c, Reward: 0.0\n",
      "Step 172: Events: c, Reward: 0.0\n",
      "Step 173: Events: c, Reward: 0.0\n",
      "Step 174: Events: c, Reward: 0.0\n",
      "Step 175: Events: c, Reward: 0.0\n",
      "Step 176: Events: c, Reward: 0.0\n",
      "Step 177: Events: c, Reward: 0.0\n",
      "Step 178: Events: c, Reward: 0.0\n",
      "Step 179: Events: c, Reward: 0.0\n",
      "Step 180: Events: c, Reward: 0.0\n",
      "Step 181: Events: c, Reward: 0.0\n",
      "Step 182: Events: c, Reward: 0.0\n",
      "Step 183: Events: c, Reward: 0.0\n",
      "Step 184: Events: c, Reward: 0.0\n",
      "Step 185: Events: c, Reward: 0.0\n",
      "Step 186: Events: c, Reward: 0.0\n",
      "Step 187: Events: c, Reward: 0.0\n",
      "Step 188: Events: c, Reward: 0.0\n",
      "Step 189: Events: c, Reward: 0.0\n",
      "Step 190: Events: c, Reward: 0.0\n",
      "Step 191: Events: c, Reward: 0.0\n",
      "Step 192: Events: c, Reward: 0.0\n",
      "Step 193: Events: c, Reward: 0.0\n",
      "Step 194: Events: c, Reward: 0.0\n",
      "Step 195: Events: c, Reward: 0.0\n",
      "Step 196: Events: c, Reward: 0.0\n",
      "Step 197: Events: c, Reward: 0.0\n",
      "Step 198: Events: c, Reward: 0.0\n",
      "Step 199: Events: cm, Reward: 1.0\n",
      "Episode finished. Total reward: 1.0\n",
      "\n",
      "Episode 3\n",
      "Step 0: Events: c, Reward: 0.0\n",
      "Step 1: Events: c, Reward: 0.0\n",
      "Step 2: Events: c, Reward: 0.0\n",
      "Step 3: Events: c, Reward: 0.0\n",
      "Step 4: Events: c, Reward: 0.0\n",
      "Step 5: Events: c, Reward: 0.0\n",
      "Step 6: Events: c, Reward: 0.0\n",
      "Step 7: Events: c, Reward: 0.0\n",
      "Step 8: Events: c, Reward: 0.0\n",
      "Step 9: Events: c, Reward: 0.0\n",
      "Step 10: Events: c, Reward: 0.0\n",
      "Step 11: Events: c, Reward: 0.0\n",
      "Step 12: Events: c, Reward: 0.0\n",
      "Step 13: Events: c, Reward: 0.0\n",
      "Step 14: Events: c, Reward: 0.0\n",
      "Step 15: Events: c, Reward: 0.0\n",
      "Step 16: Events: ch, Reward: 0\n",
      "Episode finished. Total reward: 0.0\n",
      "\n",
      "Episode 4\n",
      "Step 0: Events: c, Reward: 0.0\n",
      "Step 1: Events: c, Reward: 0.0\n",
      "Step 2: Events: c, Reward: 0.0\n",
      "Step 3: Events: c, Reward: 0.0\n",
      "Step 4: Events: c, Reward: 0.0\n",
      "Step 5: Events: c, Reward: 0.0\n",
      "Step 6: Events: c, Reward: 0.0\n",
      "Step 7: Events: c, Reward: 0.0\n",
      "Step 8: Events: c, Reward: 0.0\n",
      "Step 9: Events: c, Reward: 0.0\n",
      "Step 10: Events: c, Reward: 0.0\n",
      "Step 11: Events: c, Reward: 0.0\n",
      "Step 12: Events: c, Reward: 0.0\n",
      "Step 13: Events: c, Reward: 0.0\n",
      "Step 14: Events: c, Reward: 0.0\n",
      "Step 15: Events: c, Reward: 0.0\n",
      "Step 16: Events: c, Reward: 0.0\n",
      "Step 17: Events: c, Reward: 0.0\n",
      "Step 18: Events: c, Reward: 0.0\n",
      "Step 19: Events: c, Reward: 0.0\n",
      "Step 20: Events: c, Reward: 0.0\n",
      "Step 21: Events: c, Reward: 0.0\n",
      "Step 22: Events: c, Reward: 0.0\n",
      "Step 23: Events: c, Reward: 0.0\n",
      "Step 24: Events: c, Reward: 0.0\n",
      "Step 25: Events: c, Reward: 0.0\n",
      "Step 26: Events: c, Reward: 0.0\n",
      "Step 27: Events: c, Reward: 0.0\n",
      "Step 28: Events: c, Reward: 0.0\n",
      "Step 29: Events: c, Reward: 0.0\n",
      "Step 30: Events: c, Reward: 0.0\n",
      "Step 31: Events: c, Reward: 0.0\n",
      "Step 32: Events: c, Reward: 0.0\n",
      "Step 33: Events: c, Reward: 0.0\n",
      "Step 34: Events: c, Reward: 0.0\n",
      "Step 35: Events: c, Reward: 0.0\n",
      "Step 36: Events: c, Reward: 0.0\n",
      "Step 37: Events: c, Reward: 0.0\n",
      "Step 38: Events: c, Reward: 0.0\n",
      "Step 39: Events: c, Reward: 0.0\n",
      "Step 40: Events: c, Reward: 0.0\n",
      "Step 41: Events: c, Reward: 0.0\n",
      "Step 42: Events: c, Reward: 0.0\n",
      "Step 43: Events: c, Reward: 0.0\n",
      "Step 44: Events: c, Reward: 0.0\n",
      "Step 45: Events: c, Reward: 0.0\n",
      "Step 46: Events: c, Reward: 0.0\n",
      "Step 47: Events: c, Reward: 0.0\n",
      "Step 48: Events: c, Reward: 0.0\n",
      "Step 49: Events: c, Reward: 0.0\n",
      "Step 50: Events: c, Reward: 0.0\n",
      "Step 51: Events: c, Reward: 0.0\n",
      "Step 52: Events: c, Reward: 0.0\n",
      "Step 53: Events: c, Reward: 0.0\n",
      "Step 54: Events: c, Reward: 0.0\n",
      "Step 55: Events: c, Reward: 0.0\n",
      "Step 56: Events: c, Reward: 0.0\n",
      "Step 57: Events: c, Reward: 0.0\n",
      "Step 58: Events: c, Reward: 0.0\n",
      "Step 59: Events: c, Reward: 0.0\n",
      "Step 60: Events: c, Reward: 0.0\n",
      "Step 61: Events: c, Reward: 0.0\n",
      "Step 62: Events: c, Reward: 0.0\n",
      "Step 63: Events: c, Reward: 0.0\n",
      "Step 64: Events: c, Reward: 0.0\n",
      "Step 65: Events: c, Reward: 0.0\n",
      "Step 66: Events: c, Reward: 0.0\n",
      "Step 67: Events: c, Reward: 0.0\n",
      "Step 68: Events: c, Reward: 0.0\n",
      "Step 69: Events: c, Reward: 0.0\n",
      "Step 70: Events: c, Reward: 0.0\n",
      "Step 71: Events: c, Reward: 0.0\n",
      "Step 72: Events: c, Reward: 0.0\n",
      "Step 73: Events: c, Reward: 0.0\n",
      "Step 74: Events: c, Reward: 0.0\n",
      "Step 75: Events: c, Reward: 0.0\n",
      "Step 76: Events: c, Reward: 0.0\n",
      "Step 77: Events: c, Reward: 0.0\n",
      "Step 78: Events: c, Reward: 0.0\n",
      "Step 79: Events: c, Reward: 0.0\n",
      "Step 80: Events: c, Reward: 0.0\n",
      "Step 81: Events: c, Reward: 0.0\n",
      "Step 82: Events: c, Reward: 0.0\n",
      "Step 83: Events: c, Reward: 0.0\n",
      "Step 84: Events: c, Reward: 0.0\n",
      "Step 85: Events: c, Reward: 0.0\n",
      "Step 86: Events: c, Reward: 0.0\n",
      "Step 87: Events: c, Reward: 0.0\n",
      "Step 88: Events: c, Reward: 0.0\n",
      "Step 89: Events: c, Reward: 0.0\n",
      "Step 90: Events: c, Reward: 0.0\n",
      "Step 91: Events: c, Reward: 0.0\n",
      "Step 92: Events: c, Reward: 0.0\n",
      "Step 93: Events: c, Reward: 0.0\n",
      "Step 94: Events: c, Reward: 0.0\n",
      "Step 95: Events: c, Reward: 0.0\n",
      "Step 96: Events: c, Reward: 0.0\n",
      "Step 97: Events: c, Reward: 0.0\n",
      "Step 98: Events: c, Reward: 0.0\n",
      "Step 99: Events: c, Reward: 0.0\n",
      "Step 100: Events: c, Reward: 0.0\n",
      "Step 101: Events: c, Reward: 0.0\n",
      "Step 102: Events: c, Reward: 0.0\n",
      "Step 103: Events: c, Reward: 0.0\n",
      "Step 104: Events: c, Reward: 0.0\n",
      "Step 105: Events: c, Reward: 0.0\n",
      "Step 106: Events: c, Reward: 0.0\n",
      "Step 107: Events: c, Reward: 0.0\n",
      "Step 108: Events: c, Reward: 0.0\n",
      "Step 109: Events: c, Reward: 0.0\n",
      "Step 110: Events: c, Reward: 0.0\n",
      "Step 111: Events: c, Reward: 0.0\n",
      "Step 112: Events: c, Reward: 0.0\n",
      "Step 113: Events: c, Reward: 0.0\n",
      "Step 114: Events: c, Reward: 0.0\n",
      "Step 115: Events: c, Reward: 0.0\n",
      "Step 116: Events: c, Reward: 0.0\n",
      "Step 117: Events: c, Reward: 0.0\n",
      "Step 118: Events: c, Reward: 0.0\n",
      "Step 119: Events: c, Reward: 0.0\n",
      "Step 120: Events: c, Reward: 0.0\n",
      "Step 121: Events: c, Reward: 0.0\n",
      "Step 122: Events: c, Reward: 0.0\n",
      "Step 123: Events: c, Reward: 0.0\n",
      "Step 124: Events: c, Reward: 0.0\n",
      "Step 125: Events: c, Reward: 0.0\n",
      "Step 126: Events: c, Reward: 0.0\n",
      "Step 127: Events: c, Reward: 0.0\n",
      "Step 128: Events: c, Reward: 0.0\n",
      "Step 129: Events: c, Reward: 0.0\n",
      "Step 130: Events: c, Reward: 0.0\n",
      "Step 131: Events: c, Reward: 0.0\n",
      "Step 132: Events: c, Reward: 0.0\n",
      "Step 133: Events: c, Reward: 0.0\n",
      "Step 134: Events: c, Reward: 0.0\n",
      "Step 135: Events: c, Reward: 0.0\n",
      "Step 136: Events: c, Reward: 0.0\n",
      "Step 137: Events: c, Reward: 0.0\n",
      "Step 138: Events: c, Reward: 0.0\n",
      "Step 139: Events: c, Reward: 0.0\n",
      "Step 140: Events: c, Reward: 0.0\n",
      "Step 141: Events: c, Reward: 0.0\n",
      "Step 142: Events: c, Reward: 0.0\n",
      "Step 143: Events: c, Reward: 0.0\n",
      "Step 144: Events: c, Reward: 0.0\n",
      "Step 145: Events: c, Reward: 0.0\n",
      "Step 146: Events: c, Reward: 0.0\n",
      "Step 147: Events: c, Reward: 0.0\n",
      "Step 148: Events: c, Reward: 0.0\n",
      "Step 149: Events: c, Reward: 0.0\n",
      "Step 150: Events: c, Reward: 0.0\n",
      "Step 151: Events: c, Reward: 0.0\n",
      "Step 152: Events: c, Reward: 0.0\n",
      "Step 153: Events: c, Reward: 0.0\n",
      "Step 154: Events: c, Reward: 0.0\n",
      "Step 155: Events: c, Reward: 0.0\n",
      "Step 156: Events: c, Reward: 0.0\n",
      "Step 157: Events: c, Reward: 0.0\n",
      "Step 158: Events: c, Reward: 0.0\n",
      "Step 159: Events: c, Reward: 0.0\n",
      "Step 160: Events: c, Reward: 0.0\n",
      "Step 161: Events: c, Reward: 0.0\n",
      "Step 162: Events: c, Reward: 0.0\n",
      "Step 163: Events: c, Reward: 0.0\n",
      "Step 164: Events: c, Reward: 0.0\n",
      "Step 165: Events: c, Reward: 0.0\n",
      "Step 166: Events: c, Reward: 0.0\n",
      "Step 167: Events: c, Reward: 0.0\n",
      "Step 168: Events: c, Reward: 0.0\n",
      "Step 169: Events: c, Reward: 0.0\n",
      "Step 170: Events: c, Reward: 0.0\n",
      "Step 171: Events: c, Reward: 0.0\n",
      "Step 172: Events: c, Reward: 0.0\n",
      "Step 173: Events: c, Reward: 0.0\n",
      "Step 174: Events: c, Reward: 0.0\n",
      "Step 175: Events: c, Reward: 0.0\n",
      "Step 176: Events: c, Reward: 0.0\n",
      "Step 177: Events: c, Reward: 0.0\n",
      "Step 178: Events: c, Reward: 0.0\n",
      "Step 179: Events: c, Reward: 0.0\n",
      "Step 180: Events: c, Reward: 0.0\n",
      "Step 181: Events: c, Reward: 0.0\n",
      "Step 182: Events: c, Reward: 0.0\n",
      "Step 183: Events: c, Reward: 0.0\n",
      "Step 184: Events: c, Reward: 0.0\n",
      "Step 185: Events: c, Reward: 0.0\n",
      "Step 186: Events: c, Reward: 0.0\n",
      "Step 187: Events: c, Reward: 0.0\n",
      "Step 188: Events: c, Reward: 0.0\n",
      "Step 189: Events: c, Reward: 0.0\n",
      "Step 190: Events: c, Reward: 0.0\n",
      "Step 191: Events: c, Reward: 0.0\n",
      "Step 192: Events: c, Reward: 0.0\n",
      "Step 193: Events: c, Reward: 0.0\n",
      "Step 194: Events: c, Reward: 0.0\n",
      "Step 195: Events: c, Reward: 0.0\n",
      "Step 196: Events: c, Reward: 0.0\n",
      "Step 197: Events: c, Reward: 0.0\n",
      "Step 198: Events: c, Reward: 0.0\n",
      "Step 199: Events: cm, Reward: 1.0\n",
      "Episode finished. Total reward: 1.0\n",
      "\n",
      "Episode 5\n",
      "Step 0: Events: c, Reward: 0.0\n",
      "Step 1: Events: c, Reward: 0.0\n",
      "Step 2: Events: c, Reward: 0.0\n",
      "Step 3: Events: c, Reward: 0.0\n",
      "Step 4: Events: c, Reward: 0.0\n",
      "Step 5: Events: c, Reward: 0.0\n",
      "Step 6: Events: c, Reward: 0.0\n",
      "Step 7: Events: c, Reward: 0.0\n",
      "Step 8: Events: c, Reward: 0.0\n",
      "Step 9: Events: c, Reward: 0.0\n",
      "Step 10: Events: c, Reward: 0.0\n",
      "Step 11: Events: c, Reward: 0.0\n",
      "Step 12: Events: c, Reward: 0.0\n",
      "Step 13: Events: c, Reward: 0.0\n",
      "Step 14: Events: c, Reward: 0.0\n",
      "Step 15: Events: c, Reward: 0.0\n",
      "Step 16: Events: c, Reward: 0.0\n",
      "Step 17: Events: c, Reward: 0.0\n",
      "Step 18: Events: c, Reward: 0.0\n",
      "Step 19: Events: c, Reward: 0.0\n",
      "Step 20: Events: c, Reward: 0.0\n",
      "Step 21: Events: c, Reward: 0.0\n",
      "Step 22: Events: c, Reward: 0.0\n",
      "Step 23: Events: c, Reward: 0.0\n",
      "Step 24: Events: c, Reward: 0.0\n",
      "Step 25: Events: c, Reward: 0.0\n",
      "Step 26: Events: c, Reward: 0.0\n",
      "Step 27: Events: c, Reward: 0.0\n",
      "Step 28: Events: c, Reward: 0.0\n",
      "Step 29: Events: c, Reward: 0.0\n",
      "Step 30: Events: c, Reward: 0.0\n",
      "Step 31: Events: c, Reward: 0.0\n",
      "Step 32: Events: c, Reward: 0.0\n",
      "Step 33: Events: c, Reward: 0.0\n",
      "Step 34: Events: c, Reward: 0.0\n",
      "Step 35: Events: c, Reward: 0.0\n",
      "Step 36: Events: c, Reward: 0.0\n",
      "Step 37: Events: c, Reward: 0.0\n",
      "Step 38: Events: c, Reward: 0.0\n",
      "Step 39: Events: c, Reward: 0.0\n",
      "Step 40: Events: c, Reward: 0.0\n",
      "Step 41: Events: c, Reward: 0.0\n",
      "Step 42: Events: c, Reward: 0.0\n",
      "Step 43: Events: c, Reward: 0.0\n",
      "Step 44: Events: c, Reward: 0.0\n",
      "Step 45: Events: c, Reward: 0.0\n",
      "Step 46: Events: c, Reward: 0.0\n",
      "Step 47: Events: c, Reward: 0.0\n",
      "Step 48: Events: c, Reward: 0.0\n",
      "Step 49: Events: c, Reward: 0.0\n",
      "Step 50: Events: c, Reward: 0.0\n",
      "Step 51: Events: c, Reward: 0.0\n",
      "Step 52: Events: c, Reward: 0.0\n",
      "Step 53: Events: c, Reward: 0.0\n",
      "Step 54: Events: c, Reward: 0.0\n",
      "Step 55: Events: c, Reward: 0.0\n",
      "Step 56: Events: c, Reward: 0.0\n",
      "Step 57: Events: c, Reward: 0.0\n",
      "Step 58: Events: c, Reward: 0.0\n",
      "Step 59: Events: c, Reward: 0.0\n",
      "Step 60: Events: c, Reward: 0.0\n",
      "Step 61: Events: c, Reward: 0.0\n",
      "Step 62: Events: c, Reward: 0.0\n",
      "Step 63: Events: c, Reward: 0.0\n",
      "Step 64: Events: c, Reward: 0.0\n",
      "Step 65: Events: c, Reward: 0.0\n",
      "Step 66: Events: c, Reward: 0.0\n",
      "Step 67: Events: c, Reward: 0.0\n",
      "Step 68: Events: c, Reward: 0.0\n",
      "Step 69: Events: c, Reward: 0.0\n",
      "Step 70: Events: c, Reward: 0.0\n",
      "Step 71: Events: c, Reward: 0.0\n",
      "Step 72: Events: c, Reward: 0.0\n",
      "Step 73: Events: c, Reward: 0.0\n",
      "Step 74: Events: c, Reward: 0.0\n",
      "Step 75: Events: c, Reward: 0.0\n",
      "Step 76: Events: c, Reward: 0.0\n",
      "Step 77: Events: c, Reward: 0.0\n",
      "Step 78: Events: c, Reward: 0.0\n",
      "Step 79: Events: c, Reward: 0.0\n",
      "Step 80: Events: c, Reward: 0.0\n",
      "Step 81: Events: c, Reward: 0.0\n",
      "Step 82: Events: c, Reward: 0.0\n",
      "Step 83: Events: c, Reward: 0.0\n",
      "Step 84: Events: c, Reward: 0.0\n",
      "Step 85: Events: c, Reward: 0.0\n",
      "Step 86: Events: c, Reward: 0.0\n",
      "Step 87: Events: c, Reward: 0.0\n",
      "Step 88: Events: c, Reward: 0.0\n",
      "Step 89: Events: c, Reward: 0.0\n",
      "Step 90: Events: c, Reward: 0.0\n",
      "Step 91: Events: c, Reward: 0.0\n",
      "Step 92: Events: c, Reward: 0.0\n",
      "Step 93: Events: c, Reward: 0.0\n",
      "Step 94: Events: c, Reward: 0.0\n",
      "Step 95: Events: c, Reward: 0.0\n",
      "Step 96: Events: c, Reward: 0.0\n",
      "Step 97: Events: c, Reward: 0.0\n",
      "Step 98: Events: ch, Reward: 0\n",
      "Episode finished. Total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "def test_rm(env, num_episodes=5):\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step = 0 \n",
    "        max_steps=env.network_interface.game_mode.game_rules.max_steps.value \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        while not done and step < max_steps:\n",
    "            action = env.action_space.sample() # Random action\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            events = env.get_events()\n",
    "            total_reward += reward\n",
    "            print(f\"Step {step}: Events: {events}, Reward: {reward}\") \n",
    "            step += 1\n",
    "        print(f\"Episode finished. Total reward: {total_reward}\")\n",
    "# Run the test\n",
    "test_rm(YTenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n",
      "Terminal states: [2]\n",
      "Final U: [0, 1]\n",
      "delta_u: {0: {0: '!c&!m&!h', 1: 'c&!m&!h', -1: '!c&m&!h|c&m&!h'}, 1: {0: '!c&!m&!h', 1: 'c&!m&!h', -1: '!c&m&!h|c&m&!h'}}\n",
      "delta_r: {0: {0: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16856d360>, 1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16856c340>, -1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16856d0c0>}, 1: {0: <reward_machines.reward_functions.ConstantRewardFunction object at 0x16856f460>, 1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x168595f30>, -1: <reward_machines.reward_functions.ConstantRewardFunction object at 0x168594430>}}\n",
      "Debug: Created YT1RMEnv\n",
      "State 0:uncompromised nodes\n",
      "State 1:at least one compromised nodes\n",
      "State 2:max_step_reached and high_value_node not compromised\n",
      "0->0 (Expected State)  - Effective state: 0\n",
      "0->1 (Expected State)  - Effective state: 1\n",
      "1->1 (Expected State)  - Effective state: 1\n",
      "1->0 (Expected State)  - Effective state: 0\n",
      "0->2 (Blue agent wins - Expected State) - Effective state: -1\n",
      "1->2 (Blue agent wins - Expected State) - Effective state: -1\n",
      "\n",
      "Transition when high_value_node is compromised:\n",
      "\n",
      "0->exit (Blue agent lost - Expected State) - Effective state: -1\n",
      "1->exit (Blue agent lost - Expected State) - Effective state: -1\n",
      "0->exit (Blue agent lost - Expected State) - Effective state: -1\n",
      "1->exit (Blue agent lost - Expected State) - Effective state: -1\n"
     ]
    }
   ],
   "source": [
    "# In your main code, after creating the environment:\n",
    "env = YT1RMEnv()\n",
    "print(f\"Debug: Created YT1RMEnv\")\n",
    "\n",
    "print(\"State 0:uncompromised nodes\")\n",
    "print(\"State 1:at least one compromised nodes\")\n",
    "print(\"State 2:max_step_reached and high_value_node not compromised\")\n",
    "\n",
    "\n",
    "print(\"0->0 (Expected State)  - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"\"))\n",
    "print(\"0->1 (Expected State)  - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"c\"))\n",
    "print(\"1->1 (Expected State)  - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"c\"))\n",
    "print(\"1->0 (Expected State)  - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"\"))\n",
    "print(\"0->2 (Blue agent wins - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(1, \"cm\"))\n",
    "print(\"1->2 (Blue agent wins - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(1, \"cm\"))\n",
    "\n",
    "print(\"\\nTransition when high_value_node is compromised:\\n\")\n",
    "print(\"0->exit (Blue agent lost - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"ch\"))\n",
    "print(\"1->exit (Blue agent lost - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(1, \"ch\"))\n",
    "print(\"0->exit (Blue agent lost - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(0, \"h\"))\n",
    "print(\"1->exit (Blue agent lost - Expected State) - Effective state:\", env.reward_machines[0]._compute_next_state(1, \"h\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "848c8f009db9df6e0e1b3a1aa5b08bbcab98ccfd0977252ce3d23bd921ece881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
